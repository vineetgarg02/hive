PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF_txt
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF_txt
POSTHOOK: type: DROPTABLE
PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF
PREHOOK: type: DROPTABLE
POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF
POSTHOOK: type: DROPTABLE
PREHOOK: query: CREATE TABLE DECIMAL_UDF_txt (key decimal(20,10), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DECIMAL_UDF_txt
POSTHOOK: query: CREATE TABLE DECIMAL_UDF_txt (key decimal(20,10), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DECIMAL_UDF_txt
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_UDF_txt
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@decimal_udf_txt
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_UDF_txt
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@decimal_udf_txt
PREHOOK: query: CREATE TABLE DECIMAL_UDF (key decimal(20,10), value int)
STORED AS ORC
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DECIMAL_UDF
POSTHOOK: query: CREATE TABLE DECIMAL_UDF (key decimal(20,10), value int)
STORED AS ORC
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DECIMAL_UDF
PREHOOK: query: INSERT OVERWRITE TABLE DECIMAL_UDF SELECT * FROM DECIMAL_UDF_txt
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt
PREHOOK: Output: default@decimal_udf
POSTHOOK: query: INSERT OVERWRITE TABLE DECIMAL_UDF SELECT * FROM DECIMAL_UDF_txt
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt
POSTHOOK: Output: default@decimal_udf
POSTHOOK: Lineage: decimal_udf.key SIMPLE [(decimal_udf_txt)decimal_udf_txt.FieldSchema(name:key, type:decimal(20,10), comment:null), ]
POSTHOOK: Lineage: decimal_udf.value SIMPLE [(decimal_udf_txt)decimal_udf_txt.FieldSchema(name:value, type:int, comment:null), ]
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + key FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + key FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key + key) (type: decimal(21,10))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColAddDecimalColumn(col 0:decimal(20,10), col 0:decimal(20,10)) -> 3:decimal(21,10)
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(21,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + key FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + key FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-8800.0000000000
NULL
0.0000000000
0.0000000000
200.0000000000
20.0000000000
2.0000000000
0.2000000000
0.0200000000
400.0000000000
40.0000000000
4.0000000000
0.0000000000
0.4000000000
0.0400000000
0.6000000000
0.6600000000
0.6660000000
-0.6000000000
-0.6600000000
-0.6660000000
2.0000000000
4.0000000000
6.2800000000
-2.2400000000
-2.2400000000
-2.2440000000
2.2400000000
2.2440000000
248.0000000000
250.4000000000
-2510.9800000000
6.2800000000
6.2800000000
6.2800000000
2.0000000000
-2469135780.2469135780
2469135780.2469135600
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + value FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + value FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key + CAST( value AS decimal(10,0))) (type: decimal(21,10))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DecimalColAddDecimalColumn(col 0:decimal(20,10), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(21,10)
                    Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(21,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + value FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + value FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
0.0000000000
NULL
0.0000000000
0.0000000000
200.0000000000
20.0000000000
2.0000000000
0.1000000000
0.0100000000
400.0000000000
40.0000000000
4.0000000000
0.0000000000
0.2000000000
0.0200000000
0.3000000000
0.3300000000
0.3330000000
-0.3000000000
-0.3300000000
-0.3330000000
2.0000000000
4.0000000000
6.1400000000
-2.1200000000
-2.1200000000
-12.1220000000
2.1200000000
2.1220000000
248.0000000000
250.2000000000
-2510.4900000000
6.1400000000
6.1400000000
7.1400000000
2.0000000000
-2469135780.1234567890
2469135780.1234567800
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + (value/2) FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + (value/2) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) + (UDFToDouble(value) / 2.0)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColAddDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                    Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + (value/2) FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + (value/2) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-2200.0
NULL
0.0
0.0
150.0
15.0
1.5
0.1
0.01
300.0
30.0
3.0
0.0
0.2
0.02
0.3
0.33
0.333
-0.3
-0.33
-0.333
1.5
3.0
4.640000000000001
-1.62
-1.62
-6.622
1.62
1.622
186.0
187.7
-1882.99
4.640000000000001
4.640000000000001
5.140000000000001
1.5
-1.8518518351234567E9
1.8518518351234567E9
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + '1.0' FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + '1.0' FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) + 1.0) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColAddDoubleScalar(col 3:double, val 1.0)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double) -> 4:double
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + '1.0' FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + '1.0' FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-4399.0
NULL
1.0
1.0
101.0
11.0
2.0
1.1
1.01
201.0
21.0
3.0
1.0
1.2
1.02
1.3
1.33
1.333
0.7
0.6699999999999999
0.667
2.0
3.0
4.140000000000001
-0.1200000000000001
-0.1200000000000001
-0.12200000000000011
2.12
2.122
125.0
126.2
-1254.49
4.140000000000001
4.140000000000001
4.140000000000001
2.0
-1.2345678891234567E9
1.2345678911234567E9
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - key FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - key FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key - key) (type: decimal(21,10))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColSubtractDecimalColumn(col 0:decimal(20,10), col 0:decimal(20,10)) -> 3:decimal(21,10)
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(21,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - key FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - key FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
0.0000000000
NULL
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - value FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - value FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key - CAST( value AS decimal(10,0))) (type: decimal(21,10))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DecimalColSubtractDecimalColumn(col 0:decimal(20,10), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(21,10)
                    Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(21,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - value FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - value FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-8800.0000000000
NULL
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.1000000000
0.0100000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.2000000000
0.0200000000
0.3000000000
0.3300000000
0.3330000000
-0.3000000000
-0.3300000000
-0.3330000000
0.0000000000
0.0000000000
0.1400000000
-0.1200000000
-0.1200000000
9.8780000000
0.1200000000
0.1220000000
0.0000000000
0.2000000000
-0.4900000000
0.1400000000
0.1400000000
-0.8600000000
0.0000000000
-0.1234567890
0.1234567800
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - (value/2) FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - (value/2) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) - (UDFToDouble(value) / 2.0)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColSubtractDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                    Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - (value/2) FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - (value/2) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-6600.0
NULL
0.0
0.0
50.0
5.0
0.5
0.1
0.01
100.0
10.0
1.0
0.0
0.2
0.02
0.3
0.33
0.333
-0.3
-0.33
-0.333
0.5
1.0
1.6400000000000001
-0.6200000000000001
-0.6200000000000001
4.378
0.6200000000000001
0.6220000000000001
62.0
62.7
-627.99
1.6400000000000001
1.6400000000000001
1.1400000000000001
0.5
-6.172839451234567E8
6.172839451234567E8
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - '1.0' FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - '1.0' FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) - 1.0) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColSubtractDoubleScalar(col 3:double, val 1.0)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double) -> 4:double
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - '1.0' FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - '1.0' FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-4401.0
NULL
-1.0
-1.0
99.0
9.0
0.0
-0.9
-0.99
199.0
19.0
1.0
-1.0
-0.8
-0.98
-0.7
-0.6699999999999999
-0.667
-1.3
-1.33
-1.333
0.0
1.0
2.14
-2.12
-2.12
-2.122
0.1200000000000001
0.12200000000000011
123.0
124.2
-1256.49
2.14
2.14
2.14
0.0
-1.2345678911234567E9
1.2345678891234567E9
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * key FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * key FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key * key) (type: decimal(38,17))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColMultiplyDecimalColumn(col 0:decimal(20,10), col 0:decimal(20,10)) -> 3:decimal(38,17)
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(38,17)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * key FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * key FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
19360000.00000000000000000
NULL
0.00000000000000000
0.00000000000000000
10000.00000000000000000
100.00000000000000000
1.00000000000000000
0.01000000000000000
0.00010000000000000
40000.00000000000000000
400.00000000000000000
4.00000000000000000
0.00000000000000000
0.04000000000000000
0.00040000000000000
0.09000000000000000
0.10890000000000000
0.11088900000000000
0.09000000000000000
0.10890000000000000
0.11088900000000000
1.00000000000000000
4.00000000000000000
9.85960000000000000
1.25440000000000000
1.25440000000000000
1.25888400000000000
1.25440000000000000
1.25888400000000000
15376.00000000000000000
15675.04000000000000000
1576255.14010000000000000
9.85960000000000000
9.85960000000000000
9.85960000000000000
1.00000000000000000
1524157875323883675.01905199875019052
1524157875323883652.79682997652796840
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key, value FROM DECIMAL_UDF where key * value > 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key, value FROM DECIMAL_UDF where key * value > 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterDecimalColGreaterDecimalScalar(col 4:decimal(31,10), val 0)(children: DecimalColMultiplyDecimalColumn(col 0:decimal(20,10), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(31,10))
                    predicate: ((key * CAST( value AS decimal(10,0))) > 0) (type: boolean)
                    Statistics: Num rows: 12 Data size: 1392 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: decimal(20,10)), value (type: int)
                      outputColumnNames: _col0, _col1
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0, 1]
                      Statistics: Num rows: 12 Data size: 1392 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 12 Data size: 1392 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(31,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key, value FROM DECIMAL_UDF where key * value > 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key, value FROM DECIMAL_UDF where key * value > 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
100.0000000000	100
10.0000000000	10
1.0000000000	1
200.0000000000	200
20.0000000000	20
2.0000000000	2
1.0000000000	1
2.0000000000	2
3.1400000000	3
-1.1200000000	-1
-1.1200000000	-1
-1.1220000000	-11
1.1200000000	1
1.1220000000	1
124.0000000000	124
125.2000000000	125
-1255.4900000000	-1255
3.1400000000	3
3.1400000000	3
3.1400000000	4
1.0000000000	1
-1234567890.1234567890	-1234567890
1234567890.1234567800	1234567890
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * value FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * value FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key * CAST( value AS decimal(10,0))) (type: decimal(31,10))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DecimalColMultiplyDecimalColumn(col 0:decimal(20,10), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(31,10)
                    Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(31,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * value FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * value FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-19360000.0000000000
NULL
0.0000000000
0.0000000000
10000.0000000000
100.0000000000
1.0000000000
0.0000000000
0.0000000000
40000.0000000000
400.0000000000
4.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
0.0000000000
1.0000000000
4.0000000000
9.4200000000
1.1200000000
1.1200000000
12.3420000000
1.1200000000
1.1220000000
15376.0000000000
15650.0000000000
1575639.9500000000
9.4200000000
9.4200000000
12.5600000000
1.0000000000
1524157875171467887.5019052100
1524157875171467876.3907942000
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * (value/2) FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * (value/2) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) * (UDFToDouble(value) / 2.0)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColMultiplyDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                    Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * (value/2) FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * (value/2) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-9680000.0
NULL
0.0
0.0
5000.0
50.0
0.5
0.0
0.0
20000.0
200.0
2.0
0.0
0.0
0.0
0.0
0.0
0.0
-0.0
-0.0
-0.0
0.5
2.0
4.71
0.56
0.56
6.171
0.56
0.561
7688.0
7825.0
787819.975
4.71
4.71
6.28
0.5
7.6207893758573389E17
7.6207893758573389E17
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * '2.0' FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * '2.0' FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) * 2.0) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColMultiplyDoubleScalar(col 3:double, val 2.0)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double) -> 4:double
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * '2.0' FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * '2.0' FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-8800.0
NULL
0.0
0.0
200.0
20.0
2.0
0.2
0.02
400.0
40.0
4.0
0.0
0.4
0.04
0.6
0.66
0.666
-0.6
-0.66
-0.666
2.0
4.0
6.28
-2.24
-2.24
-2.244
2.24
2.244
248.0
250.4
-2510.98
6.28
6.28
6.28
2.0
-2.4691357802469134E9
2.4691357802469134E9
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / 0 FROM DECIMAL_UDF limit 1
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / 0 FROM DECIMAL_UDF limit 1
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key / 0) (type: decimal(22,12))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColDivideDecimalScalar(col 0:decimal(20,10), val 0) -> 3:decimal(22,12)
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    Limit
                      Number of rows: 1
                      Limit Vectorization:
                          className: VectorLimitOperator
                          native: true
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(22,12)]

  Stage: Stage-0
    Fetch Operator
      limit: 1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / 0 FROM DECIMAL_UDF limit 1
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / 0 FROM DECIMAL_UDF limit 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / key FROM DECIMAL_UDF WHERE key is not null and key <> 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / key FROM DECIMAL_UDF WHERE key is not null and key <> 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterDecimalColNotEqualDecimalScalar(col 0:decimal(20,10), val 0)
                    predicate: (key <> 0) (type: boolean)
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: (key / key) (type: decimal(38,18))
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [3]
                          selectExpressions: DecimalColDivideDecimalColumn(col 0:decimal(20,10), col 0:decimal(20,10)) -> 3:decimal(38,18)
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(38,18)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / key FROM DECIMAL_UDF WHERE key is not null and key <> 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / key FROM DECIMAL_UDF WHERE key is not null and key <> 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
1.000000000000000000
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / value FROM DECIMAL_UDF WHERE value is not null and value <> 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / value FROM DECIMAL_UDF WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterLongColNotEqualLongScalar(col 1:int, val 0)
                    predicate: (value <> 0) (type: boolean)
                    Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: (key / CAST( value AS decimal(10,0))) (type: decimal(31,21))
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [4]
                          selectExpressions: DecimalColDivideDecimalColumn(col 0:decimal(20,10), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(31,21)
                      Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(31,21)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / value FROM DECIMAL_UDF WHERE value is not null and value <> 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / value FROM DECIMAL_UDF WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.000000000000000000000
1.046666666666666666667
1.120000000000000000000
1.120000000000000000000
0.102000000000000000000
1.120000000000000000000
1.122000000000000000000
1.000000000000000000000
1.001600000000000000000
1.000390438247011952191
1.046666666666666666667
1.046666666666666666667
0.785000000000000000000
1.000000000000000000000
1.000000000100000000000
1.000000000099999992710
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / (value/2) FROM DECIMAL_UDF  WHERE value is not null and value <> 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / (value/2) FROM DECIMAL_UDF  WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterLongColNotEqualLongScalar(col 1:int, val 0)
                    predicate: (value <> 0) (type: boolean)
                    Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: (UDFToDouble(key) / (UDFToDouble(value) / 2.0)) (type: double)
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [4]
                          selectExpressions: DoubleColDivideDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                      Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / (value/2) FROM DECIMAL_UDF  WHERE value is not null and value <> 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / (value/2) FROM DECIMAL_UDF  WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0933333333333333
2.24
2.24
0.20400000000000001
2.24
2.244
2.0
2.0032
2.000780876494024
2.0933333333333333
2.0933333333333333
1.57
2.0
2.0000000002
2.0000000002
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT 1 + (key / '2.0') FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT 1 + (key / '2.0') FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (1.0 + (UDFToDouble(key) / 2.0)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DoubleScalarAddDoubleColumn(val 1.0, col 4:double)(children: DoubleColDivideDoubleScalar(col 3:double, val 2.0)(children: CastDecimalToDouble(col 0:decimal(20,10)) -> 3:double) -> 4:double) -> 3:double
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT 1 + (key / '2.0') FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT 1 + (key / '2.0') FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-2199.0
NULL
1.0
1.0
51.0
6.0
1.5
1.05
1.005
101.0
11.0
2.0
1.0
1.1
1.01
1.15
1.165
1.1665
0.85
0.835
0.8335
1.5
2.0
2.5700000000000003
0.43999999999999995
0.43999999999999995
0.43899999999999995
1.56
1.561
63.0
63.6
-626.745
2.5700000000000003
2.5700000000000003
2.5700000000000003
1.5
-6.172839440617284E8
6.172839460617284E8
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT abs(key) FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT abs(key) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: abs(key) (type: decimal(20,10))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncAbsDecimalToDecimal(col 0:decimal(20,10)) -> 3:decimal(20,10)
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(20,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT abs(key) FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT abs(key) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
4400.0000000000
NULL
0.0000000000
0.0000000000
100.0000000000
10.0000000000
1.0000000000
0.1000000000
0.0100000000
200.0000000000
20.0000000000
2.0000000000
0.0000000000
0.2000000000
0.0200000000
0.3000000000
0.3300000000
0.3330000000
0.3000000000
0.3300000000
0.3330000000
1.0000000000
2.0000000000
3.1400000000
1.1200000000
1.1200000000
1.1220000000
1.1200000000
1.1220000000
124.0000000000
125.2000000000
1255.4900000000
3.1400000000
3.1400000000
3.1400000000
1.0000000000
1234567890.1234567890
1234567890.1234567800
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF GROUP BY value ORDER BY value
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF GROUP BY value ORDER BY value
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(20,10)), value (type: int)
                    outputColumnNames: key, value
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0, 1]
                    Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: sum(key), count(key), avg(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFSumDecimal(col 0:decimal(20,10)) -> decimal(30,10), VectorUDAFCount(col 0:decimal(20,10)) -> bigint, VectorUDAFAvgDecimal(col 0:decimal(20,10)) -> struct<count:bigint,sum:decimal(30,10),input:decimal(20,10)>
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 1:int
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1, 2]
                      keys: value (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            keyColumnNums: [0]
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [1, 2, 3]
                        Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: decimal(30,10)), _col2 (type: bigint), _col3 (type: struct<count:bigint,sum:decimal(30,10),input:decimal(20,10)>)
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 4
                    dataColumns: KEY._col0:int, VALUE._col0:decimal(30,10), VALUE._col1:bigint, VALUE._col2:struct<count:bigint,sum:decimal(30,10),input:decimal(20,10)>
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), count(VALUE._col1), avg(VALUE._col2)
                Group By Vectorization:
                    aggregators: VectorUDAFSumDecimal(col 1:decimal(30,10)) -> decimal(30,10), VectorUDAFCountMerge(col 2:bigint) -> bigint, VectorUDAFAvgDecimalFinal(col 3:struct<count:bigint,sum:decimal(30,10),input:decimal(20,10)>) -> decimal(24,14)
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:int
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1, 2]
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 19 Data size: 2204 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: int), (_col1 / CAST( _col2 AS decimal(19,0))) (type: decimal(38,18)), _col3 (type: decimal(24,14)), _col1 (type: decimal(30,10))
                  outputColumnNames: _col0, _col1, _col2, _col3
                  Select Vectorization:
                      className: VectorSelectOperator
                      native: true
                      projectedOutputColumnNums: [0, 5, 3, 1]
                      selectExpressions: DecimalColDivideDecimalColumn(col 1:decimal(30,10), col 4:decimal(19,0))(children: CastLongToDecimal(col 2:bigint) -> 4:decimal(19,0)) -> 5:decimal(38,18)
                  Statistics: Num rows: 19 Data size: 2204 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: int)
                    sort order: +
                    Reduce Sink Vectorization:
                        className: VectorReduceSinkObjectHashOperator
                        keyColumnNums: [0]
                        native: true
                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                        valueColumnNums: [5, 3, 1]
                    Statistics: Num rows: 19 Data size: 2204 Basic stats: COMPLETE Column stats: NONE
                    value expressions: _col1 (type: decimal(38,18)), _col2 (type: decimal(24,14)), _col3 (type: decimal(30,10))
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 4
                    dataColumns: KEY.reducesinkkey0:int, VALUE._col0:decimal(38,18), VALUE._col1:decimal(24,14), VALUE._col2:decimal(30,10)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: decimal(38,18)), VALUE._col1 (type: decimal(24,14)), VALUE._col2 (type: decimal(30,10))
                outputColumnNames: _col0, _col1, _col2, _col3
                Select Vectorization:
                    className: VectorSelectOperator
                    native: true
                    projectedOutputColumnNums: [0, 1, 2, 3]
                Statistics: Num rows: 19 Data size: 2204 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 19 Data size: 2204 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF GROUP BY value ORDER BY value
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF GROUP BY value ORDER BY value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-1234567890	-1234567890.123456789000000000	-1234567890.12345678900000	-1234567890.1234567890
-1255	-1255.490000000000000000	-1255.49000000000000	-1255.4900000000
-11	-1.122000000000000000	-1.12200000000000	-1.1220000000
-1	-1.120000000000000000	-1.12000000000000	-2.2400000000
0	0.025384615384615385	0.02538461538462	0.3300000000
1	1.048400000000000000	1.04840000000000	5.2420000000
2	2.000000000000000000	2.00000000000000	4.0000000000
3	3.140000000000000000	3.14000000000000	9.4200000000
4	3.140000000000000000	3.14000000000000	3.1400000000
10	10.000000000000000000	10.00000000000000	10.0000000000
20	20.000000000000000000	20.00000000000000	20.0000000000
100	100.000000000000000000	100.00000000000000	100.0000000000
124	124.000000000000000000	124.00000000000000	124.0000000000
125	125.200000000000000000	125.20000000000000	125.2000000000
200	200.000000000000000000	200.00000000000000	200.0000000000
4400	-4400.000000000000000000	-4400.00000000000000	-4400.0000000000
1234567890	1234567890.123456780000000000	1234567890.12345678000000	1234567890.1234567800
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT -key FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT -key FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (- key) (type: decimal(20,10))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncNegateDecimalToDecimal(col 0:decimal(20,10)) -> 3:decimal(20,10)
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(20,10)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT -key FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT -key FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
4400.0000000000
NULL
0.0000000000
0.0000000000
-100.0000000000
-10.0000000000
-1.0000000000
-0.1000000000
-0.0100000000
-200.0000000000
-20.0000000000
-2.0000000000
0.0000000000
-0.2000000000
-0.0200000000
-0.3000000000
-0.3300000000
-0.3330000000
0.3000000000
0.3300000000
0.3330000000
-1.0000000000
-2.0000000000
-3.1400000000
1.1200000000
1.1200000000
1.1220000000
-1.1200000000
-1.1220000000
-124.0000000000
-125.2000000000
1255.4900000000
-3.1400000000
-3.1400000000
-3.1400000000
-1.0000000000
1234567890.1234567890
-1234567890.1234567800
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT +key FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT +key FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: decimal_udf
          Select Operator
            expressions: key (type: decimal(20,10))
            outputColumnNames: _col0
            ListSink

PREHOOK: query: SELECT +key FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT +key FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-4400.0000000000
NULL
0.0000000000
0.0000000000
100.0000000000
10.0000000000
1.0000000000
0.1000000000
0.0100000000
200.0000000000
20.0000000000
2.0000000000
0.0000000000
0.2000000000
0.0200000000
0.3000000000
0.3300000000
0.3330000000
-0.3000000000
-0.3300000000
-0.3330000000
1.0000000000
2.0000000000
3.1400000000
-1.1200000000
-1.1200000000
-1.1220000000
1.1200000000
1.1220000000
124.0000000000
125.2000000000
-1255.4900000000
3.1400000000
3.1400000000
3.1400000000
1.0000000000
-1234567890.1234567890
1234567890.1234567800
PREHOOK: query: EXPlAIN SELECT CEIL(key) FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPlAIN SELECT CEIL(key) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: ceil(key) (type: decimal(11,0))
                    outputColumnNames: _col0
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT CEIL(key) FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT CEIL(key) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-4400
NULL
0
0
100
10
1
1
1
200
20
2
0
1
1
1
1
1
0
0
0
1
2
4
-1
-1
-1
2
2
124
126
-1255
4
4
4
1
-1234567890
1234567891
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT FLOOR(key) FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT FLOOR(key) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: floor(key) (type: decimal(11,0))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncFloorDecimalToDecimal(col 0:decimal(20,10)) -> 3:decimal(11,0)
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(11,0)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT FLOOR(key) FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT FLOOR(key) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-4400
NULL
0
0
100
10
1
0
0
200
20
2
0
0
0
0
0
0
-1
-1
-1
1
2
3
-2
-2
-2
1
1
124
125
-1256
3
3
3
1
-1234567891
1234567890
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT ROUND(key, 2) FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT ROUND(key, 2) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: round(key, 2) (type: decimal(13,2))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncRoundWithNumDigitsDecimalToDecimal(col 0:decimal(20,10), decimalPlaces 2) -> 3:decimal(13,2)
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(13,2)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT ROUND(key, 2) FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT ROUND(key, 2) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-4400.00
NULL
0.00
0.00
100.00
10.00
1.00
0.10
0.01
200.00
20.00
2.00
0.00
0.20
0.02
0.30
0.33
0.33
-0.30
-0.33
-0.33
1.00
2.00
3.14
-1.12
-1.12
-1.12
1.12
1.12
124.00
125.20
-1255.49
3.14
3.14
3.14
1.00
-1234567890.12
1234567890.12
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT POWER(key, 2) FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT POWER(key, 2) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: power(key, 2) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: VectorUDFAdaptor(power(key, 2)) -> 3:double
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: true
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT POWER(key, 2) FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT POWER(key, 2) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
1.936E7
NULL
0.0
0.0
10000.0
100.0
1.0
0.010000000000000002
1.0E-4
40000.0
400.0
4.0
0.0
0.04000000000000001
4.0E-4
0.09
0.10890000000000001
0.11088900000000002
0.09
0.10890000000000001
0.11088900000000002
1.0
4.0
9.8596
1.2544000000000002
1.2544000000000002
1.2588840000000003
1.2544000000000002
1.2588840000000003
15376.0
15675.04
1576255.1401
9.8596
9.8596
9.8596
1.0
1.52415787532388352E18
1.52415787532388352E18
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: ((key + 1) % (key / 2)) (type: decimal(22,12))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [5]
                        selectExpressions: DecimalColModuloDecimalColumn(col 3:decimal(21,10), col 4:decimal(22,12))(children: DecimalColAddDecimalScalar(col 0:decimal(20,10), val 1) -> 3:decimal(21,10), DecimalColDivideDecimalScalar(col 0:decimal(20,10), val 2) -> 4:decimal(22,12)) -> 5:decimal(22,12)
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(21,10), decimal(22,12), decimal(22,12)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-2199.000000000000
NULL
NULL
NULL
1.000000000000
1.000000000000
0.000000000000
0.000000000000
0.000000000000
1.000000000000
1.000000000000
0.000000000000
NULL
0.000000000000
0.000000000000
0.100000000000
0.010000000000
0.001000000000
0.100000000000
0.010000000000
0.001000000000
0.000000000000
0.000000000000
1.000000000000
-0.120000000000
-0.120000000000
-0.122000000000
0.440000000000
0.439000000000
1.000000000000
1.000000000000
-626.745000000000
1.000000000000
1.000000000000
1.000000000000
0.000000000000
-617283944.061728394500
1.000000000000
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF GROUP BY value
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF GROUP BY value
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(20,10)), value (type: int)
                    outputColumnNames: key, value
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0, 1]
                    Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: stddev(key), variance(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFVarDecimal(col 0:decimal(20,10)) -> struct<count:bigint,sum:double,variance:double> aggregation: stddev, VectorUDAFVarDecimal(col 0:decimal(20,10)) -> struct<count:bigint,sum:double,variance:double> aggregation: variance
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 1:int
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1]
                      keys: value (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            keyColumnNums: [0]
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [1, 2]
                        Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>)
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 3
                    dataColumns: KEY._col0:int, VALUE._col0:struct<count:bigint,sum:double,variance:double>, VALUE._col1:struct<count:bigint,sum:double,variance:double>
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: stddev(VALUE._col0), variance(VALUE._col1)
                Group By Vectorization:
                    aggregators: VectorUDAFVarFinal(col 1:struct<count:bigint,sum:double,variance:double>) -> double aggregation: stddev, VectorUDAFVarFinal(col 2:struct<count:bigint,sum:double,variance:double>) -> double aggregation: variance
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:int
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1]
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 19 Data size: 2204 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 19 Data size: 2204 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF GROUP BY value
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF GROUP BY value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
4	0.0	0.0
-1234567890	0.0	0.0
0	0.22561046704494161	0.050900082840236685
1	0.05928102563215321	0.0035142400000000066
2	0.0	0.0
3	0.0	0.0
124	0.0	0.0
200	0.0	0.0
4400	0.0	0.0
1234567890	0.0	0.0
10	0.0	0.0
125	0.0	0.0
-1255	0.0	0.0
-11	0.0	0.0
-1	0.0	0.0
20	0.0	0.0
100	0.0	0.0
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF GROUP BY value
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF GROUP BY value
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(20,10)), value (type: int)
                    outputColumnNames: key, value
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0, 1]
                    Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: stddev_samp(key), var_samp(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFVarDecimal(col 0:decimal(20,10)) -> struct<count:bigint,sum:double,variance:double> aggregation: stddev_samp, VectorUDAFVarDecimal(col 0:decimal(20,10)) -> struct<count:bigint,sum:double,variance:double> aggregation: var_samp
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 1:int
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1]
                      keys: value (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            keyColumnNums: [0]
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [1, 2]
                        Statistics: Num rows: 38 Data size: 4408 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>)
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 3
                    dataColumns: KEY._col0:int, VALUE._col0:struct<count:bigint,sum:double,variance:double>, VALUE._col1:struct<count:bigint,sum:double,variance:double>
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: stddev_samp(VALUE._col0), var_samp(VALUE._col1)
                Group By Vectorization:
                    aggregators: VectorUDAFVarFinal(col 1:struct<count:bigint,sum:double,variance:double>) -> double aggregation: stddev_samp, VectorUDAFVarFinal(col 2:struct<count:bigint,sum:double,variance:double>) -> double aggregation: var_samp
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:int
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1]
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 19 Data size: 2204 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 19 Data size: 2204 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF GROUP BY value
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF GROUP BY value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
4	NULL	NULL
-1234567890	NULL	NULL
0	0.2348228191855647	0.055141756410256405
1	0.06627820154470102	0.004392800000000008
2	0.0	0.0
3	0.0	0.0
124	NULL	NULL
200	NULL	NULL
4400	NULL	NULL
1234567890	NULL	NULL
10	NULL	NULL
125	NULL	NULL
-1255	NULL	NULL
-11	NULL	NULL
-1	0.0	0.0
20	NULL	NULL
100	NULL	NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: decimal(20,10))
                    outputColumnNames: _col0
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: histogram_numeric(_col0, 3)
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 272 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Statistics: Num rows: 1 Data size: 272 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: array<double>)
            Execution mode: llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                notVectorizedReason: Aggregation Function expression for GROUPBY operator: UDF histogram_numeric not supported
                vectorized: false
        Reducer 2 
            Execution mode: llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: Aggregation Function expression for GROUPBY operator: UDF histogram_numeric not supported
                vectorized: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: histogram_numeric(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 832 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 832 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
[{"x":-1.2345678901234567E9,"y":1.0},{"x":-144.50057142857142,"y":35.0},{"x":1.2345678901234567E9,"y":1.0}]
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MIN(key) FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MIN(key) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(20,10))
                    outputColumnNames: key
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0]
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: min(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFMinDecimal(col 0:decimal(20,10)) -> decimal(20,10)
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0]
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0]
                        Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: decimal(20,10))
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 1
                    dataColumns: VALUE._col0:decimal(20,10)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0)
                Group By Vectorization:
                    aggregators: VectorUDAFMinDecimal(col 0:decimal(20,10)) -> decimal(20,10)
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0]
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT MIN(key) FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT MIN(key) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
-1234567890.1234567890
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MAX(key) FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MAX(key) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(20,10))
                    outputColumnNames: key
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0]
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: max(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFMaxDecimal(col 0:decimal(20,10)) -> decimal(20,10)
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0]
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0]
                        Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: decimal(20,10))
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 1
                    dataColumns: VALUE._col0:decimal(20,10)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0)
                Group By Vectorization:
                    aggregators: VectorUDAFMaxDecimal(col 0:decimal(20,10)) -> decimal(20,10)
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0]
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT MAX(key) FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT MAX(key) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
1234567890.1234567800
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT COUNT(key) FROM DECIMAL_UDF
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT COUNT(key) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf
                  Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(20,10), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(20,10))
                    outputColumnNames: key
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0]
                    Statistics: Num rows: 38 Data size: 4256 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFCount(col 0:decimal(20,10)) -> bigint
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0]
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 120 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0]
                        Statistics: Num rows: 1 Data size: 120 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: all inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true
                inputFormatFeatureSupport: []
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(20,10), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 1
                    dataColumns: VALUE._col0:bigint
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                Group By Vectorization:
                    aggregators: VectorUDAFCountMerge(col 0:bigint) -> bigint
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0]
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 120 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 120 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT COUNT(key) FROM DECIMAL_UDF
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
POSTHOOK: query: SELECT COUNT(key) FROM DECIMAL_UDF
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf
#### A masked pattern was here ####
37
PREHOOK: query: CREATE TABLE DECIMAL_UDF_txt_small (key decimal(15,3), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE
PREHOOK: type: CREATETABLE
PREHOOK: Output: database:default
PREHOOK: Output: default@DECIMAL_UDF_txt_small
POSTHOOK: query: CREATE TABLE DECIMAL_UDF_txt_small (key decimal(15,3), value int)
ROW FORMAT DELIMITED
   FIELDS TERMINATED BY ' '
STORED AS TEXTFILE
POSTHOOK: type: CREATETABLE
POSTHOOK: Output: database:default
POSTHOOK: Output: default@DECIMAL_UDF_txt_small
PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_UDF_txt_small
PREHOOK: type: LOAD
#### A masked pattern was here ####
PREHOOK: Output: default@decimal_udf_txt_small
POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/kv7.txt' INTO TABLE DECIMAL_UDF_txt_small
POSTHOOK: type: LOAD
#### A masked pattern was here ####
POSTHOOK: Output: default@decimal_udf_txt_small
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key + key) (type: decimal(16,3))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColAddDecimalColumn(col 0:decimal(15,3), col 0:decimal(15,3)) -> 3:decimal(16,3)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(16,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-8800.000
NULL
0.000
0.000
200.000
20.000
2.000
0.200
0.020
400.000
40.000
4.000
0.000
0.400
0.040
0.600
0.660
0.666
-0.600
-0.660
-0.666
2.000
4.000
6.280
-2.240
-2.240
-2.244
2.240
2.244
248.000
250.400
-2510.980
6.280
6.280
6.280
2.000
-2469135780.246
2469135780.246
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + value FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + value FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key + CAST( value AS decimal(10,0))) (type: decimal(16,3))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DecimalColAddDecimalColumn(col 0:decimal(15,3), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(16,3)
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(16,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + value FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + value FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
0.000
NULL
0.000
0.000
200.000
20.000
2.000
0.100
0.010
400.000
40.000
4.000
0.000
0.200
0.020
0.300
0.330
0.333
-0.300
-0.330
-0.333
2.000
4.000
6.140
-2.120
-2.120
-12.122
2.120
2.122
248.000
250.200
-2510.490
6.140
6.140
7.140
2.000
-2469135780.123
2469135780.123
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + (value/2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + (value/2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) + (UDFToDouble(value) / 2.0)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColAddDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + (value/2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + (value/2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-2200.0
NULL
0.0
0.0
150.0
15.0
1.5
0.1
0.01
300.0
30.0
3.0
0.0
0.2
0.02
0.3
0.33
0.333
-0.3
-0.33
-0.333
1.5
3.0
4.640000000000001
-1.62
-1.62
-6.622
1.62
1.622
186.0
187.7
-1882.99
4.640000000000001
4.640000000000001
5.140000000000001
1.5
-1.851851835123E9
1.851851835123E9
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + '1.0' FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key + '1.0' FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) + 1.0) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColAddDoubleScalar(col 3:double, val 1.0)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double) -> 4:double
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key + '1.0' FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key + '1.0' FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-4399.0
NULL
1.0
1.0
101.0
11.0
2.0
1.1
1.01
201.0
21.0
3.0
1.0
1.2
1.02
1.3
1.33
1.333
0.7
0.6699999999999999
0.667
2.0
3.0
4.140000000000001
-0.1200000000000001
-0.1200000000000001
-0.12200000000000011
2.12
2.122
125.0
126.2
-1254.49
4.140000000000001
4.140000000000001
4.140000000000001
2.0
-1.234567889123E9
1.234567891123E9
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key - key) (type: decimal(16,3))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColSubtractDecimalColumn(col 0:decimal(15,3), col 0:decimal(15,3)) -> 3:decimal(16,3)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(16,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
0.000
NULL
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - value FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - value FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key - CAST( value AS decimal(10,0))) (type: decimal(16,3))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DecimalColSubtractDecimalColumn(col 0:decimal(15,3), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(16,3)
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(16,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - value FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - value FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-8800.000
NULL
0.000
0.000
0.000
0.000
0.000
0.100
0.010
0.000
0.000
0.000
0.000
0.200
0.020
0.300
0.330
0.333
-0.300
-0.330
-0.333
0.000
0.000
0.140
-0.120
-0.120
9.878
0.120
0.122
0.000
0.200
-0.490
0.140
0.140
-0.860
0.000
-0.123
0.123
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - (value/2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - (value/2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) - (UDFToDouble(value) / 2.0)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColSubtractDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - (value/2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - (value/2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-6600.0
NULL
0.0
0.0
50.0
5.0
0.5
0.1
0.01
100.0
10.0
1.0
0.0
0.2
0.02
0.3
0.33
0.333
-0.3
-0.33
-0.333
0.5
1.0
1.6400000000000001
-0.6200000000000001
-0.6200000000000001
4.378
0.6200000000000001
0.6220000000000001
62.0
62.7
-627.99
1.6400000000000001
1.6400000000000001
1.1400000000000001
0.5
-6.172839451229999E8
6.172839451229999E8
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - '1.0' FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key - '1.0' FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) - 1.0) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColSubtractDoubleScalar(col 3:double, val 1.0)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double) -> 4:double
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key - '1.0' FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key - '1.0' FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-4401.0
NULL
-1.0
-1.0
99.0
9.0
0.0
-0.9
-0.99
199.0
19.0
1.0
-1.0
-0.8
-0.98
-0.7
-0.6699999999999999
-0.667
-1.3
-1.33
-1.333
0.0
1.0
2.14
-2.12
-2.12
-2.122
0.1200000000000001
0.12200000000000011
123.0
124.2
-1256.49
2.14
2.14
2.14
0.0
-1.234567891123E9
1.234567889123E9
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key * key) (type: decimal(31,6))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColMultiplyDecimalColumn(col 0:decimal(15,3), col 0:decimal(15,3)) -> 3:decimal(31,6)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(31,6)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
19360000.000000
NULL
0.000000
0.000000
10000.000000
100.000000
1.000000
0.010000
0.000100
40000.000000
400.000000
4.000000
0.000000
0.040000
0.000400
0.090000
0.108900
0.110889
0.090000
0.108900
0.110889
1.000000
4.000000
9.859600
1.254400
1.254400
1.258884
1.254400
1.258884
15376.000000
15675.040000
1576255.140100
9.859600
9.859600
9.859600
1.000000
1524157875322755800.955129
1524157875322755800.955129
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key, value FROM DECIMAL_UDF_txt_small where key * value > 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key, value FROM DECIMAL_UDF_txt_small where key * value > 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterDecimalColGreaterDecimalScalar(col 4:decimal(26,3), val 0)(children: DecimalColMultiplyDecimalColumn(col 0:decimal(15,3), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(26,3))
                    predicate: ((key * CAST( value AS decimal(10,0))) > 0) (type: boolean)
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: key (type: decimal(15,3)), value (type: int)
                      outputColumnNames: _col0, _col1
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [0, 1]
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(26,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key, value FROM DECIMAL_UDF_txt_small where key * value > 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key, value FROM DECIMAL_UDF_txt_small where key * value > 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
100.000	100
10.000	10
1.000	1
200.000	200
20.000	20
2.000	2
1.000	1
2.000	2
3.140	3
-1.120	-1
-1.120	-1
-1.122	-11
1.120	1
1.122	1
124.000	124
125.200	125
-1255.490	-1255
3.140	3
3.140	3
3.140	4
1.000	1
-1234567890.123	-1234567890
1234567890.123	1234567890
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * value FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * value FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key * CAST( value AS decimal(10,0))) (type: decimal(26,3))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DecimalColMultiplyDecimalColumn(col 0:decimal(15,3), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(26,3)
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(26,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * value FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * value FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-19360000.000
NULL
0.000
0.000
10000.000
100.000
1.000
0.000
0.000
40000.000
400.000
4.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
1.000
4.000
9.420
1.120
1.120
12.342
1.120
1.122
15376.000
15650.000
1575639.950
9.420
9.420
12.560
1.000
1524157875170903950.470
1524157875170903950.470
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * (value/2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * (value/2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) * (UDFToDouble(value) / 2.0)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColMultiplyDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * (value/2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * (value/2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-9680000.0
NULL
0.0
0.0
5000.0
50.0
0.5
0.0
0.0
20000.0
200.0
2.0
0.0
0.0
0.0
0.0
0.0
0.0
-0.0
-0.0
-0.0
0.5
2.0
4.71
0.56
0.56
6.171
0.56
0.561
7688.0
7825.0
787819.975
4.71
4.71
6.28
0.5
7.620789375854519E17
7.620789375854519E17
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * '2.0' FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key * '2.0' FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (UDFToDouble(key) * 2.0) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [4]
                        selectExpressions: DoubleColMultiplyDoubleScalar(col 3:double, val 2.0)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double) -> 4:double
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key * '2.0' FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key * '2.0' FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-8800.0
NULL
0.0
0.0
200.0
20.0
2.0
0.2
0.02
400.0
40.0
4.0
0.0
0.4
0.04
0.6
0.66
0.666
-0.6
-0.66
-0.666
2.0
4.0
6.28
-2.24
-2.24
-2.244
2.24
2.244
248.0
250.4
-2510.98
6.28
6.28
6.28
2.0
-2.469135780246E9
2.469135780246E9
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / 0 FROM DECIMAL_UDF_txt_small limit 1
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / 0 FROM DECIMAL_UDF_txt_small limit 1
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (key / 0) (type: decimal(18,6))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DecimalColDivideDecimalScalar(col 0:decimal(15,3), val 0) -> 3:decimal(18,6)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    Limit
                      Number of rows: 1
                      Limit Vectorization:
                          className: VectorLimitOperator
                          native: true
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(18,6)]

  Stage: Stage-0
    Fetch Operator
      limit: 1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / 0 FROM DECIMAL_UDF_txt_small limit 1
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / 0 FROM DECIMAL_UDF_txt_small limit 1
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / key FROM DECIMAL_UDF_txt_small WHERE key is not null and key <> 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / key FROM DECIMAL_UDF_txt_small WHERE key is not null and key <> 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterDecimalColNotEqualDecimalScalar(col 0:decimal(15,3), val 0)
                    predicate: (key <> 0) (type: boolean)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: (key / key) (type: decimal(34,19))
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [3]
                          selectExpressions: DecimalColDivideDecimalColumn(col 0:decimal(15,3), col 0:decimal(15,3)) -> 3:decimal(34,19)
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(34,19)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / key FROM DECIMAL_UDF_txt_small WHERE key is not null and key <> 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / key FROM DECIMAL_UDF_txt_small WHERE key is not null and key <> 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
1.0000000000000000000
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / value FROM DECIMAL_UDF_txt_small WHERE value is not null and value <> 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / value FROM DECIMAL_UDF_txt_small WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterLongColNotEqualLongScalar(col 1:int, val 0)
                    predicate: (value <> 0) (type: boolean)
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: (key / CAST( value AS decimal(10,0))) (type: decimal(26,14))
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [4]
                          selectExpressions: DecimalColDivideDecimalColumn(col 0:decimal(15,3), col 3:decimal(10,0))(children: CastLongToDecimal(col 1:int) -> 3:decimal(10,0)) -> 4:decimal(26,14)
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(10,0), decimal(26,14)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / value FROM DECIMAL_UDF_txt_small WHERE value is not null and value <> 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / value FROM DECIMAL_UDF_txt_small WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-1.00000000000000
1.00000000000000
1.00000000000000
1.00000000000000
1.00000000000000
1.00000000000000
1.00000000000000
1.00000000000000
1.00000000000000
1.04666666666667
1.12000000000000
1.12000000000000
0.10200000000000
1.12000000000000
1.12200000000000
1.00000000000000
1.00160000000000
1.00039043824701
1.04666666666667
1.04666666666667
0.78500000000000
1.00000000000000
1.00000000009963
1.00000000009963
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / (value/2) FROM DECIMAL_UDF_txt_small  WHERE value is not null and value <> 0
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT key / (value/2) FROM DECIMAL_UDF_txt_small  WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Filter Operator
                    Filter Vectorization:
                        className: VectorFilterOperator
                        native: true
                        predicateExpression: FilterLongColNotEqualLongScalar(col 1:int, val 0)
                    predicate: (value <> 0) (type: boolean)
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    Select Operator
                      expressions: (UDFToDouble(key) / (UDFToDouble(value) / 2.0)) (type: double)
                      outputColumnNames: _col0
                      Select Vectorization:
                          className: VectorSelectOperator
                          native: true
                          projectedOutputColumnNums: [4]
                          selectExpressions: DoubleColDivideDoubleColumn(col 3:double, col 5:double)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double, DoubleColDivideDoubleScalar(col 4:double, val 2.0)(children: CastLongToDouble(col 1:int) -> 4:double) -> 5:double) -> 4:double
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      File Output Operator
                        compressed: false
                        File Sink Vectorization:
                            className: VectorFileSinkOperator
                            native: false
                        Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                        table:
                            input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                            output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT key / (value/2) FROM DECIMAL_UDF_txt_small  WHERE value is not null and value <> 0
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT key / (value/2) FROM DECIMAL_UDF_txt_small  WHERE value is not null and value <> 0
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0
2.0933333333333333
2.24
2.24
0.20400000000000001
2.24
2.244
2.0
2.0032
2.000780876494024
2.0933333333333333
2.0933333333333333
1.57
2.0
2.0000000001992597
2.0000000001992597
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT 1 + (key / '2.0') FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT 1 + (key / '2.0') FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (1.0 + (UDFToDouble(key) / 2.0)) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: DoubleScalarAddDoubleColumn(val 1.0, col 4:double)(children: DoubleColDivideDoubleScalar(col 3:double, val 2.0)(children: CastDecimalToDouble(col 0:decimal(15,3)) -> 3:double) -> 4:double) -> 3:double
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double, double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT 1 + (key / '2.0') FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT 1 + (key / '2.0') FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-2199.0
NULL
1.0
1.0
51.0
6.0
1.5
1.05
1.005
101.0
11.0
2.0
1.0
1.1
1.01
1.15
1.165
1.1665
0.85
0.835
0.8335
1.5
2.0
2.5700000000000003
0.43999999999999995
0.43999999999999995
0.43899999999999995
1.56
1.561
63.0
63.6
-626.745
2.5700000000000003
2.5700000000000003
2.5700000000000003
1.5
-6.172839440615E8
6.172839460615E8
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT abs(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT abs(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: abs(key) (type: decimal(15,3))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncAbsDecimalToDecimal(col 0:decimal(15,3)) -> 3:decimal(15,3)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(15,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT abs(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT abs(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
4400.000
NULL
0.000
0.000
100.000
10.000
1.000
0.100
0.010
200.000
20.000
2.000
0.000
0.200
0.020
0.300
0.330
0.333
0.300
0.330
0.333
1.000
2.000
3.140
1.120
1.120
1.122
1.120
1.122
124.000
125.200
1255.490
3.140
3.140
3.140
1.000
1234567890.123
1234567890.123
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF_txt_small GROUP BY value ORDER BY value
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF_txt_small GROUP BY value ORDER BY value
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(15,3)), value (type: int)
                    outputColumnNames: key, value
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0, 1]
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: sum(key), count(key), avg(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFSumDecimal(col 0:decimal(15,3)) -> decimal(25,3), VectorUDAFCount(col 0:decimal(15,3)) -> bigint, VectorUDAFAvgDecimal(col 0:decimal(15,3)) -> struct<count:bigint,sum:decimal(25,3),input:decimal(15,3)>
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 1:int
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1, 2]
                      keys: value (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2, _col3
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            keyColumnNums: [0]
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [1, 2, 3]
                        Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: decimal(25,3)), _col2 (type: bigint), _col3 (type: struct<count:bigint,sum:decimal(25,3),input:decimal(15,3)>)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 4
                    dataColumns: KEY._col0:int, VALUE._col0:decimal(25,3), VALUE._col1:bigint, VALUE._col2:struct<count:bigint,sum:decimal(25,3),input:decimal(15,3)>
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: sum(VALUE._col0), count(VALUE._col1), avg(VALUE._col2)
                Group By Vectorization:
                    aggregators: VectorUDAFSumDecimal(col 1:decimal(25,3)) -> decimal(25,3), VectorUDAFCountMerge(col 2:bigint) -> bigint, VectorUDAFAvgDecimalFinal(col 3:struct<count:bigint,sum:decimal(25,3),input:decimal(15,3)>) -> decimal(19,7)
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:int
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1, 2]
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2, _col3
                Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                Select Operator
                  expressions: _col0 (type: int), (_col1 / CAST( _col2 AS decimal(19,0))) (type: decimal(38,16)), _col3 (type: decimal(19,7)), _col1 (type: decimal(25,3))
                  outputColumnNames: _col0, _col1, _col2, _col3
                  Select Vectorization:
                      className: VectorSelectOperator
                      native: true
                      projectedOutputColumnNums: [0, 5, 3, 1]
                      selectExpressions: DecimalColDivideDecimalColumn(col 1:decimal(25,3), col 4:decimal(19,0))(children: CastLongToDecimal(col 2:bigint) -> 4:decimal(19,0)) -> 5:decimal(38,16)
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  Reduce Output Operator
                    key expressions: _col0 (type: int)
                    sort order: +
                    Reduce Sink Vectorization:
                        className: VectorReduceSinkObjectHashOperator
                        keyColumnNums: [0]
                        native: true
                        nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                        valueColumnNums: [5, 3, 1]
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    value expressions: _col1 (type: decimal(38,16)), _col2 (type: decimal(19,7)), _col3 (type: decimal(25,3))
        Reducer 3 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 4
                    dataColumns: KEY.reducesinkkey0:int, VALUE._col0:decimal(38,16), VALUE._col1:decimal(19,7), VALUE._col2:decimal(25,3)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Select Operator
                expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: decimal(38,16)), VALUE._col1 (type: decimal(19,7)), VALUE._col2 (type: decimal(25,3))
                outputColumnNames: _col0, _col1, _col2, _col3
                Select Vectorization:
                    className: VectorSelectOperator
                    native: true
                    projectedOutputColumnNums: [0, 1, 2, 3]
                Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF_txt_small GROUP BY value ORDER BY value
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT value, sum(key) / count(key), avg(key), sum(key) FROM DECIMAL_UDF_txt_small GROUP BY value ORDER BY value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-1234567890	-1234567890.1230000000000000	-1234567890.1230000	-1234567890.123
-1255	-1255.4900000000000000	-1255.4900000	-1255.490
-11	-1.1220000000000000	-1.1220000	-1.122
-1	-1.1200000000000000	-1.1200000	-2.240
0	0.0253846153846154	0.0253846	0.330
1	1.0484000000000000	1.0484000	5.242
2	2.0000000000000000	2.0000000	4.000
3	3.1400000000000000	3.1400000	9.420
4	3.1400000000000000	3.1400000	3.140
10	10.0000000000000000	10.0000000	10.000
20	20.0000000000000000	20.0000000	20.000
100	100.0000000000000000	100.0000000	100.000
124	124.0000000000000000	124.0000000	124.000
125	125.2000000000000000	125.2000000	125.200
200	200.0000000000000000	200.0000000	200.000
4400	-4400.0000000000000000	-4400.0000000	-4400.000
1234567890	1234567890.1230000000000000	1234567890.1230000	1234567890.123
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT -key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT -key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: (- key) (type: decimal(15,3))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncNegateDecimalToDecimal(col 0:decimal(15,3)) -> 3:decimal(15,3)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(15,3)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT -key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT -key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
4400.000
NULL
0.000
0.000
-100.000
-10.000
-1.000
-0.100
-0.010
-200.000
-20.000
-2.000
0.000
-0.200
-0.020
-0.300
-0.330
-0.333
0.300
0.330
0.333
-1.000
-2.000
-3.140
1.120
1.120
1.122
-1.120
-1.122
-124.000
-125.200
1255.490
-3.140
-3.140
-3.140
-1.000
1234567890.123
-1234567890.123
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT +key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT +key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        TableScan
          alias: decimal_udf_txt_small
          Select Operator
            expressions: key (type: decimal(15,3))
            outputColumnNames: _col0
            ListSink

PREHOOK: query: SELECT +key FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT +key FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-4400.000
NULL
0.000
0.000
100.000
10.000
1.000
0.100
0.010
200.000
20.000
2.000
0.000
0.200
0.020
0.300
0.330
0.333
-0.300
-0.330
-0.333
1.000
2.000
3.140
-1.120
-1.120
-1.122
1.120
1.122
124.000
125.200
-1255.490
3.140
3.140
3.140
1.000
-1234567890.123
1234567890.123
PREHOOK: query: EXPlAIN SELECT CEIL(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPlAIN SELECT CEIL(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: ceil(key) (type: decimal(13,0))
                    outputColumnNames: _col0
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT CEIL(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT CEIL(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-4400
NULL
0
0
100
10
1
1
1
200
20
2
0
1
1
1
1
1
0
0
0
1
2
4
-1
-1
-1
2
2
124
126
-1255
4
4
4
1
-1234567890
1234567891
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT FLOOR(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT FLOOR(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: floor(key) (type: decimal(13,0))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncFloorDecimalToDecimal(col 0:decimal(15,3)) -> 3:decimal(13,0)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(13,0)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT FLOOR(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT FLOOR(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-4400
NULL
0
0
100
10
1
0
0
200
20
2
0
0
0
0
0
0
-1
-1
-1
1
2
3
-2
-2
-2
1
1
124
125
-1256
3
3
3
1
-1234567891
1234567890
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT ROUND(key, 2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT ROUND(key, 2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: round(key, 2) (type: decimal(15,2))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: FuncRoundWithNumDigitsDecimalToDecimal(col 0:decimal(15,3), decimalPlaces 2) -> 3:decimal(15,2)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(15,2)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT ROUND(key, 2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT ROUND(key, 2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-4400.00
NULL
0.00
0.00
100.00
10.00
1.00
0.10
0.01
200.00
20.00
2.00
0.00
0.20
0.02
0.30
0.33
0.33
-0.30
-0.33
-0.33
1.00
2.00
3.14
-1.12
-1.12
-1.12
1.12
1.12
124.00
125.20
-1255.49
3.14
3.14
3.14
1.00
-1234567890.12
1234567890.12
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT POWER(key, 2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT POWER(key, 2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: power(key, 2) (type: double)
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [3]
                        selectExpressions: VectorUDFAdaptor(power(key, 2)) -> 3:double
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: true
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [double]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT POWER(key, 2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT POWER(key, 2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
1.936E7
NULL
0.0
0.0
10000.0
100.0
1.0
0.010000000000000002
1.0E-4
40000.0
400.0
4.0
0.0
0.04000000000000001
4.0E-4
0.09
0.10890000000000001
0.11088900000000002
0.09
0.10890000000000001
0.11088900000000002
1.0
4.0
9.8596
1.2544000000000002
1.2544000000000002
1.2588840000000003
1.2544000000000002
1.2588840000000003
15376.0
15675.04
1576255.1401
9.8596
9.8596
9.8596
1.0
1.52415787532275558E18
1.52415787532275558E18
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: ((key + 1) % (key / 2)) (type: decimal(18,6))
                    outputColumnNames: _col0
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [5]
                        selectExpressions: DecimalColModuloDecimalColumn(col 3:decimal(16,3), col 4:decimal(18,6))(children: DecimalColAddDecimalScalar(col 0:decimal(15,3), val 1) -> 3:decimal(16,3), DecimalColDivideDecimalScalar(col 0:decimal(15,3), val 2) -> 4:decimal(18,6)) -> 5:decimal(18,6)
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    File Output Operator
                      compressed: false
                      File Sink Vectorization:
                          className: VectorFileSinkOperator
                          native: false
                      Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                      table:
                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: [decimal(16,3), decimal(18,6), decimal(18,6)]

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT (key + 1) % (key / 2) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-2199.000000
NULL
NULL
NULL
1.000000
1.000000
0.000000
0.000000
0.000000
1.000000
1.000000
0.000000
NULL
0.000000
0.000000
0.100000
0.010000
0.001000
0.100000
0.010000
0.001000
0.000000
0.000000
1.000000
-0.120000
-0.120000
-0.122000
0.440000
0.439000
1.000000
1.000000
-626.745000
1.000000
1.000000
1.000000
0.000000
-617283944.061500
1.000000
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF_txt_small GROUP BY value
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF_txt_small GROUP BY value
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(15,3)), value (type: int)
                    outputColumnNames: key, value
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0, 1]
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: stddev(key), variance(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFVarDecimal(col 0:decimal(15,3)) -> struct<count:bigint,sum:double,variance:double> aggregation: stddev, VectorUDAFVarDecimal(col 0:decimal(15,3)) -> struct<count:bigint,sum:double,variance:double> aggregation: variance
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 1:int
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1]
                      keys: value (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            keyColumnNums: [0]
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [1, 2]
                        Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 3
                    dataColumns: KEY._col0:int, VALUE._col0:struct<count:bigint,sum:double,variance:double>, VALUE._col1:struct<count:bigint,sum:double,variance:double>
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: stddev(VALUE._col0), variance(VALUE._col1)
                Group By Vectorization:
                    aggregators: VectorUDAFVarFinal(col 1:struct<count:bigint,sum:double,variance:double>) -> double aggregation: stddev, VectorUDAFVarFinal(col 2:struct<count:bigint,sum:double,variance:double>) -> double aggregation: variance
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:int
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1]
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF_txt_small GROUP BY value
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT value, stddev(key), variance(key) FROM DECIMAL_UDF_txt_small GROUP BY value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
4	0.0	0.0
-1234567890	0.0	0.0
0	0.22561046704494161	0.050900082840236685
1	0.05928102563215321	0.0035142400000000066
2	0.0	0.0
3	0.0	0.0
124	0.0	0.0
200	0.0	0.0
4400	0.0	0.0
1234567890	0.0	0.0
10	0.0	0.0
125	0.0	0.0
-1255	0.0	0.0
-11	0.0	0.0
-1	0.0	0.0
20	0.0	0.0
100	0.0	0.0
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF_txt_small GROUP BY value
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF_txt_small GROUP BY value
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(15,3)), value (type: int)
                    outputColumnNames: key, value
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0, 1]
                    Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: stddev_samp(key), var_samp(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFVarDecimal(col 0:decimal(15,3)) -> struct<count:bigint,sum:double,variance:double> aggregation: stddev_samp, VectorUDAFVarDecimal(col 0:decimal(15,3)) -> struct<count:bigint,sum:double,variance:double> aggregation: var_samp
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          keyExpressions: col 1:int
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0, 1]
                      keys: value (type: int)
                      mode: hash
                      outputColumnNames: _col0, _col1, _col2
                      Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        key expressions: _col0 (type: int)
                        sort order: +
                        Map-reduce partition columns: _col0 (type: int)
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkLongOperator
                            keyColumnNums: [0]
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [1, 2]
                        Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col1 (type: struct<count:bigint,sum:double,variance:double>), _col2 (type: struct<count:bigint,sum:double,variance:double>)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0, 1]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: a
                reduceColumnSortOrder: +
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 3
                    dataColumns: KEY._col0:int, VALUE._col0:struct<count:bigint,sum:double,variance:double>, VALUE._col1:struct<count:bigint,sum:double,variance:double>
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: stddev_samp(VALUE._col0), var_samp(VALUE._col1)
                Group By Vectorization:
                    aggregators: VectorUDAFVarFinal(col 1:struct<count:bigint,sum:double,variance:double>) -> double aggregation: stddev_samp, VectorUDAFVarFinal(col 2:struct<count:bigint,sum:double,variance:double>) -> double aggregation: var_samp
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    keyExpressions: col 0:int
                    native: false
                    vectorProcessingMode: MERGE_PARTIAL
                    projectedOutputColumnNums: [0, 1]
                keys: KEY._col0 (type: int)
                mode: mergepartial
                outputColumnNames: _col0, _col1, _col2
                Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 116 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF_txt_small GROUP BY value
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT value, stddev_samp(key), var_samp(key) FROM DECIMAL_UDF_txt_small GROUP BY value
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
4	NULL	NULL
-1234567890	NULL	NULL
0	0.2348228191855647	0.055141756410256405
1	0.06627820154470102	0.004392800000000008
2	0.0	0.0
3	0.0	0.0
124	NULL	NULL
200	NULL	NULL
4400	NULL	NULL
1234567890	NULL	NULL
10	NULL	NULL
125	NULL	NULL
-1255	NULL	NULL
-11	NULL	NULL
-1	0.0	0.0
20	NULL	NULL
100	NULL	NULL
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  Select Operator
                    expressions: key (type: decimal(15,3))
                    outputColumnNames: _col0
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: histogram_numeric(_col0, 3)
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 272 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Statistics: Num rows: 1 Data size: 272 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: array<double>)
            Execution mode: llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                notVectorizedReason: Aggregation Function expression for GROUPBY operator: UDF histogram_numeric not supported
                vectorized: false
        Reducer 2 
            Execution mode: llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                notVectorizedReason: Aggregation Function expression for GROUPBY operator: UDF histogram_numeric not supported
                vectorized: false
            Reduce Operator Tree:
              Group By Operator
                aggregations: histogram_numeric(VALUE._col0)
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 832 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  Statistics: Num rows: 1 Data size: 832 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT histogram_numeric(key, 3) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
[{"x":-1.234567890123E9,"y":1.0},{"x":-144.50057142857142,"y":35.0},{"x":1.234567890123E9,"y":1.0}]
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MIN(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MIN(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(15,3))
                    outputColumnNames: key
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0]
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: min(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFMinDecimal(col 0:decimal(15,3)) -> decimal(15,3)
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0]
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0]
                        Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: decimal(15,3))
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 1
                    dataColumns: VALUE._col0:decimal(15,3)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: min(VALUE._col0)
                Group By Vectorization:
                    aggregators: VectorUDAFMinDecimal(col 0:decimal(15,3)) -> decimal(15,3)
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0]
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT MIN(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT MIN(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
-1234567890.123
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MAX(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT MAX(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(15,3))
                    outputColumnNames: key
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0]
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: max(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFMaxDecimal(col 0:decimal(15,3)) -> decimal(15,3)
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0]
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0]
                        Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: decimal(15,3))
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 1
                    dataColumns: VALUE._col0:decimal(15,3)
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: max(VALUE._col0)
                Group By Vectorization:
                    aggregators: VectorUDAFMaxDecimal(col 0:decimal(15,3)) -> decimal(15,3)
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0]
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 224 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT MAX(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT MAX(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
1234567890.123
PREHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT COUNT(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
POSTHOOK: query: EXPLAIN VECTORIZATION DETAIL
SELECT COUNT(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
PLAN VECTORIZATION:
  enabled: true
  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 depends on stages: Stage-1

STAGE PLANS:
  Stage: Stage-1
    Tez
#### A masked pattern was here ####
      Edges:
        Reducer 2 <- Map 1 (CUSTOM_SIMPLE_EDGE)
#### A masked pattern was here ####
      Vertices:
        Map 1 
            Map Operator Tree:
                TableScan
                  alias: decimal_udf_txt_small
                  Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                  TableScan Vectorization:
                      native: true
                      vectorizationSchemaColumns: [0:key:decimal(15,3), 1:value:int, 2:ROW__ID:struct<transactionid:bigint,bucketid:int,rowid:bigint>]
                  Select Operator
                    expressions: key (type: decimal(15,3))
                    outputColumnNames: key
                    Select Vectorization:
                        className: VectorSelectOperator
                        native: true
                        projectedOutputColumnNums: [0]
                    Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: NONE
                    Group By Operator
                      aggregations: count(key)
                      Group By Vectorization:
                          aggregators: VectorUDAFCount(col 0:decimal(15,3)) -> bigint
                          className: VectorGroupByOperator
                          groupByMode: HASH
                          native: false
                          vectorProcessingMode: HASH
                          projectedOutputColumnNums: [0]
                      mode: hash
                      outputColumnNames: _col0
                      Statistics: Num rows: 1 Data size: 120 Basic stats: COMPLETE Column stats: NONE
                      Reduce Output Operator
                        sort order: 
                        Reduce Sink Vectorization:
                            className: VectorReduceSinkEmptyKeyOperator
                            keyColumnNums: []
                            native: true
                            nativeConditionsMet: hive.vectorized.execution.reducesink.new.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true, No PTF TopN IS true, No DISTINCT columns IS true, BinarySortableSerDe for keys IS true, LazyBinarySerDe for values IS true
                            valueColumnNums: [0]
                        Statistics: Num rows: 1 Data size: 120 Basic stats: COMPLETE Column stats: NONE
                        value expressions: _col0 (type: bigint)
            Execution mode: vectorized, llap
            LLAP IO: no inputs
            Map Vectorization:
                enabled: true
                enabledConditionsMet: hive.vectorized.use.vector.serde.deserialize IS true
                inputFormatFeatureSupport: [DECIMAL_64]
                vectorizationSupportRemovedReasons: [DECIMAL_64 disabled because LLAP is enabled]
                featureSupportInUse: []
                inputFileFormats: org.apache.hadoop.mapred.TextInputFormat
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 2
                    includeColumns: [0]
                    dataColumns: key:decimal(15,3), value:int
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
        Reducer 2 
            Execution mode: vectorized, llap
            Reduce Vectorization:
                enabled: true
                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine tez IN [tez, spark] IS true
                reduceColumnNullOrder: 
                reduceColumnSortOrder: 
                allNative: false
                usesVectorUDFAdaptor: false
                vectorized: true
                rowBatchContext:
                    dataColumnCount: 1
                    dataColumns: VALUE._col0:bigint
                    partitionColumnCount: 0
                    scratchColumnTypeNames: []
            Reduce Operator Tree:
              Group By Operator
                aggregations: count(VALUE._col0)
                Group By Vectorization:
                    aggregators: VectorUDAFCountMerge(col 0:bigint) -> bigint
                    className: VectorGroupByOperator
                    groupByMode: MERGEPARTIAL
                    native: false
                    vectorProcessingMode: GLOBAL
                    projectedOutputColumnNums: [0]
                mode: mergepartial
                outputColumnNames: _col0
                Statistics: Num rows: 1 Data size: 120 Basic stats: COMPLETE Column stats: NONE
                File Output Operator
                  compressed: false
                  File Sink Vectorization:
                      className: VectorFileSinkOperator
                      native: false
                  Statistics: Num rows: 1 Data size: 120 Basic stats: COMPLETE Column stats: NONE
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe

  Stage: Stage-0
    Fetch Operator
      limit: -1
      Processor Tree:
        ListSink

PREHOOK: query: SELECT COUNT(key) FROM DECIMAL_UDF_txt_small
PREHOOK: type: QUERY
PREHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
POSTHOOK: query: SELECT COUNT(key) FROM DECIMAL_UDF_txt_small
POSTHOOK: type: QUERY
POSTHOOK: Input: default@decimal_udf_txt_small
#### A masked pattern was here ####
37
PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF_txt
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@decimal_udf_txt
PREHOOK: Output: default@decimal_udf_txt
POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF_txt
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@decimal_udf_txt
POSTHOOK: Output: default@decimal_udf_txt
PREHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF
PREHOOK: type: DROPTABLE
PREHOOK: Input: default@decimal_udf
PREHOOK: Output: default@decimal_udf
POSTHOOK: query: DROP TABLE IF EXISTS DECIMAL_UDF
POSTHOOK: type: DROPTABLE
POSTHOOK: Input: default@decimal_udf
POSTHOOK: Output: default@decimal_udf
